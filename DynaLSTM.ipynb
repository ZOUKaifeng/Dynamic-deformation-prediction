{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from psbody.mesh import Mesh\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import scipy.sparse as sp\n",
    "from generate_data import *\n",
    "from read_obj import ObjLoader, save_obj\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import transforms3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['0507','0503', '0514', '0509' ,'0512', '0501', '0901',  '1001', '4902', '4913', '4919','9302', '9305', '12411']\n",
    "data_path = \"/home/hyewon/Downloads/deep_dress/Dress dataset/\"\n",
    "\n",
    "config = { \"batch_size\" : 8,\n",
    "           \"num_features\": 204,\n",
    "           \"out_dim\": 10 ,\n",
    "           \"num_layers\": 2,\n",
    "           \"hidden_units\":16,\n",
    "           \"frame_rate\":60,\n",
    "           'learning_rate': 0.001,\n",
    "           \"num_epochs\":5000,\n",
    "           \"checkpoint_dir\" : \"./LSTM_results/0820_10/\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pose_sequences = []\n",
    "trans_sequences = []\n",
    "pc_sequences = []\n",
    "clothes_feature = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hyewon/Downloads/deep_dress/Dress dataset/0507/0507_poses.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 1/14 [00:08<01:44,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point cloud shape: (449, 4179, 3)\n",
      "data shape (449, 156)\n",
      "clothes feature shape: (449, 10)\n",
      "/home/hyewon/Downloads/deep_dress/Dress dataset/0503/0503_poses.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 2/14 [00:15<01:35,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point cloud shape: (434, 4179, 3)\n",
      "data shape (434, 156)\n",
      "clothes feature shape: (434, 10)\n",
      "/home/hyewon/Downloads/deep_dress/Dress dataset/0514/0514_poses.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██▏       | 3/14 [00:27<01:39,  9.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point cloud shape: (642, 4179, 3)\n",
      "data shape (642, 156)\n",
      "clothes feature shape: (642, 10)\n",
      "/home/hyewon/Downloads/deep_dress/Dress dataset/0509/0509_poses.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 4/14 [00:34<01:24,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point cloud shape: (399, 4179, 3)\n",
      "data shape (399, 156)\n",
      "clothes feature shape: (399, 10)\n",
      "/home/hyewon/Downloads/deep_dress/Dress dataset/0512/0512_poses.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 5/14 [00:46<01:24,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point cloud shape: (650, 4179, 3)\n",
      "data shape (650, 156)\n",
      "clothes feature shape: (650, 10)\n",
      "/home/hyewon/Downloads/deep_dress/Dress dataset/0501/0501_poses.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 6/14 [00:56<01:18,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point cloud shape: (598, 4179, 3)\n",
      "data shape (598, 156)\n",
      "clothes feature shape: (598, 10)\n",
      "/home/hyewon/Downloads/deep_dress/Dress dataset/0901/0901_poses.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 7/14 [00:59<00:53,  7.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point cloud shape: (148, 4179, 3)\n",
      "data shape (148, 156)\n",
      "clothes feature shape: (148, 10)\n",
      "/home/hyewon/Downloads/deep_dress/Dress dataset/1001/1001_poses.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 8/14 [01:13<00:58,  9.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point cloud shape: (801, 4179, 3)\n",
      "data shape (801, 156)\n",
      "clothes feature shape: (801, 10)\n",
      "/home/hyewon/Downloads/deep_dress/Dress dataset/4902/4902_poses.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 9/14 [01:24<00:50, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point cloud shape: (605, 4179, 3)\n",
      "data shape (605, 156)\n",
      "clothes feature shape: (605, 10)\n",
      "/home/hyewon/Downloads/deep_dress/Dress dataset/4913/4913_poses.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████▏  | 10/14 [01:32<00:37,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point cloud shape: (451, 4179, 3)\n",
      "data shape (451, 156)\n",
      "clothes feature shape: (451, 10)\n",
      "/home/hyewon/Downloads/deep_dress/Dress dataset/4919/4919_poses.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▊  | 11/14 [01:50<00:35, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point cloud shape: (957, 4179, 3)\n",
      "data shape (957, 156)\n",
      "clothes feature shape: (957, 10)\n",
      "/home/hyewon/Downloads/deep_dress/Dress dataset/9302/9302_poses.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 12/14 [02:18<00:33, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point cloud shape: (1492, 4179, 3)\n",
      "data shape (1492, 156)\n",
      "clothes feature shape: (1492, 10)\n",
      "/home/hyewon/Downloads/deep_dress/Dress dataset/9305/9305_poses.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 13/14 [02:28<00:14, 14.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point cloud shape: (546, 4179, 3)\n",
      "data shape (546, 156)\n",
      "clothes feature shape: (546, 10)\n",
      "/home/hyewon/Downloads/deep_dress/Dress dataset/12411/12411_poses.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:46<00:00, 11.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point cloud shape: (998, 4179, 3)\n",
      "data shape (998, 156)\n",
      "clothes feature shape: (998, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/hyewon/anaconda3/envs/img2obj/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    " for file in tqdm(files):\n",
    "\n",
    "    filepath = data_path+file+'/'+file+'_poses.npz'\n",
    "    print(filepath)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        continue\n",
    "\n",
    "    sequence = np.load(filepath, allow_pickle = True)\n",
    "    \n",
    "    pose_sequences.append(sequence['poses'][:,:66])\n",
    "\n",
    "    trans_sequences.append(sequence['trans'])\n",
    "    \n",
    "    clothes_data = np.load('./processed/0820/'+file+'.npy').squeeze()\n",
    "    clothes_feature.append(clothes_data)\n",
    "\n",
    "    obj_folder = \"/home/hyewon/Downloads/deep_dress/processed_data/\"+file+'/mesh/'\n",
    "  #  obj_list = os.listdir(obj_folder)\n",
    "    seq = []\n",
    "\n",
    "\n",
    "    for i in range(sequence['poses'].shape[0]):\n",
    "\n",
    "        rot = sequence['poses'][i]\n",
    "        trans = sequence['trans'][i]\n",
    "        filename = str(i)\n",
    "        pc = Mesh(filename = obj_folder+filename+'.obj')\n",
    "        seq.append(pc.v)\n",
    "\n",
    "    temp = np.asarray(seq).astype('float32')\n",
    "    print('point cloud shape:',temp.shape)\n",
    "    print('data shape',sequence['poses'].shape)\n",
    "    print('clothes feature shape:', clothes_data.shape)\n",
    "    pc_sequences.append(temp)\n",
    "   \n",
    "\n",
    "np.savez(\"./10_train.npz\", pc = pc_sequences, poses = pose_sequences, trans = trans_sequences, cloth = clothes_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"./10_train.npz\", allow_pickle = True)#loading data\n",
    "pose_sequences = data[\"poses\"]\n",
    "pc_sequences = data[\"pc\"]\n",
    "trans_sequences = data[\"trans\"]\n",
    "clothes_feature = data[\"cloth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(449, 4179, 3)\n",
      "(434, 4179, 3)\n",
      "(642, 4179, 3)\n",
      "(399, 4179, 3)\n",
      "(650, 4179, 3)\n",
      "(598, 4179, 3)\n",
      "(148, 4179, 3)\n",
      "(801, 4179, 3)\n",
      "(605, 4179, 3)\n",
      "(449, 4179, 3)\n",
      "(434, 4179, 3)\n",
      "(642, 4179, 3)\n",
      "(399, 4179, 3)\n",
      "(650, 4179, 3)\n",
      "(598, 4179, 3)\n",
      "(148, 4179, 3)\n",
      "(801, 4179, 3)\n",
      "(605, 4179, 3)\n",
      "(451, 4179, 3)\n",
      "(957, 4179, 3)\n",
      "(1492, 4179, 3)\n",
      "(546, 4179, 3)\n",
      "(998, 4179, 3)\n"
     ]
    }
   ],
   "source": [
    "for pc in pc_sequences:\n",
    "    print(pc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has been loaded\n",
      "computing angular velocity ......\n",
      "computing angular acceleration ......\n",
      "root velocity and acceleration ......\n"
     ]
    }
   ],
   "source": [
    "print(\"data has been loaded\")\n",
    "\n",
    "##############computing train parameters###################\n",
    "print('computing angular velocity ......')\n",
    "\n",
    "angular_velocity = compute_ang_v(pose_sequences, t)\n",
    "print('computing angular acceleration ......')\n",
    "acc = compute_acceleration(angular_velocity, t)\n",
    "print('root velocity and acceleration ......')\n",
    "trans_v = compute_root_va(trans_sequences, t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449, 66)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose_sequences[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(447, 204)\n",
      "(432, 204)\n",
      "(640, 204)\n",
      "(397, 204)\n",
      "(648, 204)\n",
      "(596, 204)\n",
      "(146, 204)\n",
      "(799, 204)\n",
      "(603, 204)\n",
      "(449, 204)\n",
      "(447, 204)\n",
      "(432, 204)\n",
      "(640, 204)\n",
      "(397, 204)\n",
      "(648, 204)\n",
      "(596, 204)\n",
      "(146, 204)\n",
      "(799, 204)\n",
      "(603, 204)\n",
      "(449, 204)\n",
      "(955, 204)\n",
      "(1490, 204)\n",
      "(544, 204)\n",
      "(996, 204)\n"
     ]
    }
   ],
   "source": [
    "#train data\n",
    "train = []\n",
    "for i, sequence in enumerate(zip(angular_velocity, trans_v)):\n",
    "    ang = np.asarray(sequence[0][1:])\n",
    "    mov = sequence[1][1:]\n",
    "    a = acc[i]\n",
    "    train_f = np.concatenate((ang, mov), axis = 1)\n",
    "    train_f = np.concatenate((train_f, a), axis = 1)\n",
    "    train_f = np.concatenate((train_f, pose_sequences[i][2:]), axis = 1)\n",
    "    train.append(train_f)\n",
    "\n",
    "for i in train:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(447, 10)\n",
      "(432, 10)\n",
      "(640, 10)\n",
      "(397, 10)\n",
      "(648, 10)\n",
      "(596, 10)\n",
      "(146, 10)\n",
      "(799, 10)\n",
      "(603, 10)\n",
      "(449, 10)\n",
      "(447, 10)\n",
      "(432, 10)\n",
      "(640, 10)\n",
      "(397, 10)\n",
      "(648, 10)\n",
      "(596, 10)\n",
      "(146, 10)\n",
      "(799, 10)\n",
      "(603, 10)\n",
      "(449, 10)\n",
      "(955, 10)\n",
      "(1490, 10)\n",
      "(544, 10)\n",
      "(996, 10)\n"
     ]
    }
   ],
   "source": [
    "#ground truth (clothes feature)\n",
    "gt = []\n",
    "for cf in clothes_feature:\n",
    "    gt.append(cf[2:, :])\n",
    "   # gt.append(cf[2:,:]-cf[1:-1,:])\n",
    "    print(gt[-1].shape)\n",
    "   # print(gt[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = [i[2:].reshape((i[2:].shape[0], -1)) for i in pc_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(447, 12537)\n",
      "(432, 12537)\n",
      "(640, 12537)\n",
      "(397, 12537)\n",
      "(648, 12537)\n",
      "(596, 12537)\n",
      "(146, 12537)\n",
      "(799, 12537)\n",
      "(603, 12537)\n",
      "(447, 12537)\n",
      "(432, 12537)\n",
      "(640, 12537)\n",
      "(397, 12537)\n",
      "(648, 12537)\n",
      "(596, 12537)\n",
      "(146, 12537)\n",
      "(799, 12537)\n",
      "(603, 12537)\n",
      "(449, 12537)\n",
      "(955, 12537)\n",
      "(1490, 12537)\n",
      "(544, 12537)\n",
      "(996, 12537)\n"
     ]
    }
   ],
   "source": [
    "for i in pc:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(447, 12537)\n",
      "(447, 204)\n",
      "(447, 10)\n",
      "(432, 12537)\n",
      "(432, 204)\n",
      "(432, 10)\n",
      "(640, 12537)\n",
      "(640, 204)\n",
      "(640, 10)\n",
      "(397, 12537)\n",
      "(397, 204)\n",
      "(397, 10)\n",
      "(648, 12537)\n",
      "(648, 204)\n",
      "(648, 10)\n",
      "(596, 12537)\n",
      "(596, 204)\n",
      "(596, 10)\n",
      "(146, 12537)\n",
      "(146, 204)\n",
      "(146, 10)\n",
      "(799, 12537)\n",
      "(799, 204)\n",
      "(799, 10)\n",
      "(603, 12537)\n",
      "(603, 204)\n",
      "(603, 10)\n",
      "(447, 12537)\n",
      "(449, 204)\n",
      "(449, 10)\n",
      "(432, 12537)\n",
      "(447, 204)\n",
      "(447, 10)\n",
      "(640, 12537)\n",
      "(432, 204)\n",
      "(432, 10)\n",
      "(397, 12537)\n",
      "(640, 204)\n",
      "(640, 10)\n",
      "(648, 12537)\n",
      "(397, 204)\n",
      "(397, 10)\n",
      "(596, 12537)\n",
      "(648, 204)\n",
      "(648, 10)\n",
      "(146, 12537)\n",
      "(596, 204)\n",
      "(596, 10)\n",
      "(799, 12537)\n",
      "(146, 204)\n",
      "(146, 10)\n",
      "(603, 12537)\n",
      "(799, 204)\n",
      "(799, 10)\n",
      "(449, 12537)\n",
      "(603, 204)\n",
      "(603, 10)\n",
      "(955, 12537)\n",
      "(449, 204)\n",
      "(449, 10)\n",
      "(1490, 12537)\n",
      "(955, 204)\n",
      "(955, 10)\n",
      "(544, 12537)\n",
      "(1490, 204)\n",
      "(1490, 10)\n",
      "(996, 12537)\n",
      "(544, 204)\n",
      "(544, 10)\n"
     ]
    }
   ],
   "source": [
    "new_train = []\n",
    "new_gt = []\n",
    "new_pc = []\n",
    "frame = 100\n",
    "for i,j,k in zip(train,gt, pc):\n",
    "    f = i.shape[0]\n",
    "    m=0\n",
    "    print(k.shape)\n",
    "    print(i.shape)\n",
    "    print(j.shape)\n",
    "    while f - m*frame > frame:\n",
    "        new_train.append(i[m*frame:(m+1)*frame])\n",
    "        new_gt.append(j[m*frame:(m+1)*frame])\n",
    "        if k[m*frame:(m+1)*frame].shape[0] != 100:\n",
    "            break\n",
    "        new_pc.append(k[m*frame:(m+1)*frame])\n",
    "        m = m+1\n",
    "    \n",
    "    \n",
    "#     if i.shape[0] != j.shape[0]:\n",
    "#         print(i[0].shape)\n",
    "#         print('error')\n",
    "#     if i.shape[0]>600:\n",
    "#         if i.shape[0]<1200 and i.shape[0]>800:\n",
    "#             new_train.append(i[:600])\n",
    "#             new_gt.append(j[:600])\n",
    "#             new_pc.append(k[:600])\n",
    "            \n",
    "#             new_train.append(i[600:])\n",
    "#             new_gt.append(j[600:])\n",
    "#             new_pc.append(k[600:])\n",
    "#         elif i.shape[0]>1200:\n",
    "#             new_train.append(i[:600])\n",
    "#             new_gt.append(j[:600])\n",
    "#             new_pc.append(k[:600])\n",
    "#             new_train.append(i[600:1200])\n",
    "#             new_gt.append(j[600:1200])\n",
    "#             new_pc.append(k[600:1200])\n",
    "            \n",
    "#     elif i.shape[0]<600:\n",
    "#         new_train.append(i)\n",
    "#         new_gt.append(j)\n",
    "#         new_pc.append(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "for i,j,k in zip(new_train, new_gt,new_pc):\n",
    "    print(i.shape[0])\n",
    "    print(j.shape[0])\n",
    "    print(k.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"./_train_LSTM.npz\", train = new_train, gt = new_gt, pc = new_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dress_data(Dataset):\n",
    "    def __init__(self, X, y, pc):\n",
    "        \n",
    "        self.X = nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=0.0)\n",
    "        self.y = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0.0)\n",
    "        print(self.X.shape)\n",
    "        self.pc = nn.utils.rnn.pad_sequence(pc, batch_first=True, padding_value=0.0)\n",
    "        #print(self.pc.shape)\n",
    "        self.file_nums = self.X.shape[0]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.file_nums\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.X[idx], self.y[idx]\n",
    "       # pc = self.pc[idx]\n",
    "        return X, y, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'num_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-283f5c86c093>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnum_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_features\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mout_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"out_dim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhidden_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hidden_units\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'num_features'"
     ]
    }
   ],
   "source": [
    "batch_size = config[\"batch_size\"]\n",
    "num_feature = config[\"num_features\"]\n",
    "num_layers = config[\"num_layers\"]\n",
    "out_dim = config[\"out_dim\"]\n",
    "hidden_units = config[\"hidden_units\"]\n",
    "learning_rate = config[\"learning_rate\"]\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "checkpoint_dir = config[\"checkpoint_dir\"]\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([79, 100, 204])\n",
      "torch.Size([4, 100, 204])\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"./8_train_LSTM.npz\", allow_pickle = True)\n",
    "train = [torch.Tensor(t) for t in data[\"train\"]]\n",
    "pc = [torch.Tensor(t) for t in data[\"pc\"]]\n",
    "gt = [torch.Tensor(t) for t in data[\"gt\"]]\n",
    "train_dataset =  Dress_data(train[4:], gt[4:], pc[4:])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = Dress_data(train[0:4], gt[0:4], pc[0:4])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100, 204])\n",
      "torch.Size([16, 100, 8])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 100, 204])\n",
      "torch.Size([16, 100, 8])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 100, 204])\n",
      "torch.Size([16, 100, 8])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 100, 204])\n",
      "torch.Size([16, 100, 8])\n",
      "torch.Size([16])\n",
      "torch.Size([15, 100, 204])\n",
      "torch.Size([15, 100, 8])\n",
      "torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "for X, y, pc in train_loader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(pc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vec_dim, hidden_dim, out_dim, num_layers, batch_size):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(vec_dim, hidden_dim, num_layers, bias = True, batch_first = True)\n",
    "\n",
    "        self.feat_layer = nn.Linear(hidden_dim, self.out_dim)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden_state_1 = self.lstm(x)\n",
    "\n",
    "        y_pred = self.feat_layer(lstm_out)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_feature, hidden_units, out_dim, num_layers, batch_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyewon/anaconda3/envs/img2obj/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = torch.nn.L1Loss(size_average=True)\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#scheduler = lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader), eta_min=learning_rate)\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "test_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load(checkpoint_dir+'result').cuda()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Coma\n",
    "from config_parser import read_config\n",
    "import argparse\n",
    "from psbody.mesh import Mesh, MeshViewers\n",
    "import mesh_operations\n",
    "from transform import Normalize\n",
    "from data import ComaDataset\n",
    "parser = argparse.ArgumentParser(description='Pytorch Trainer for Convolutional Mesh Autoencoders')\n",
    "config = read_config('./default.cfg')\n",
    "\n",
    "def scipy_to_torch_sparse(scp_matrix):\n",
    "    values = scp_matrix.data\n",
    "    indices = np.vstack((scp_matrix.row, scp_matrix.col))\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = scp_matrix.shape\n",
    "\n",
    "    sparse_tensor = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "    return sparse_tensor\n",
    "\n",
    "eval_flag = config['eval']\n",
    "lr = config['learning_rate']\n",
    "lr_decay = config['learning_rate_decay']\n",
    "weight_decay = config['weight_decay']\n",
    "total_epochs = config['epoch']\n",
    "workers_thread = config['workers_thread']\n",
    "opt = config['optimizer']\n",
    "batch_size = config['batch_size']\n",
    "val_losses, accs, durations = [], [], []\n",
    "\n",
    "\n",
    "template_mesh = Mesh(filename=\"./template/template.obj\")\n",
    "M, A, D, U = mesh_operations.generate_transform_matrices(template_mesh, config['downsampling_factors'])\n",
    "\n",
    "D_t = [scipy_to_torch_sparse(d).to(device) for d in D]\n",
    "U_t = [scipy_to_torch_sparse(u).to(device) for u in U]\n",
    "A_t = [scipy_to_torch_sparse(a).to(device) for a in A]\n",
    "num_nodes = [len(M[i].v) for i in range(len(M))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_dir = config['data_dir']\n",
    "\n",
    "normalize_transform = Normalize()\n",
    "dataset = ComaDataset(data_dir, dtype='train', split='sliced', split_term='sliced', pre_transform=normalize_transform)\n",
    "coma = Coma(dataset, config, D_t, U_t, A_t, num_nodes).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('./results/0813/checkpoint_499.pt')\n",
    "start_epoch = checkpoint['epoch_num']\n",
    "coma.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in coma.children():\n",
    "    for param in child.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train loss is 2.9651239468501163\n",
      "epoch 0: test loss is 3.7915244102478027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyewon/anaconda3/envs/img2obj/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10: train loss is 1.5319561408116267\n",
      "epoch 10: test loss is 2.145334243774414\n",
      "epoch 20: train loss is 1.3267755691821759\n",
      "epoch 20: test loss is 1.8366807699203491\n",
      "epoch 30: train loss is 1.2957450564091022\n",
      "epoch 30: test loss is 1.694142460823059\n",
      "epoch 40: train loss is 1.2855241573773897\n",
      "epoch 40: test loss is 1.6327940225601196\n",
      "epoch 50: train loss is 1.2848533116854155\n",
      "epoch 50: test loss is 1.6014702320098877\n",
      "epoch 60: train loss is 1.2738953370314379\n",
      "epoch 60: test loss is 1.5871267318725586\n",
      "epoch 70: train loss is 1.2767204963243926\n",
      "epoch 70: test loss is 1.579234004020691\n",
      "epoch 80: train loss is 1.1894661371524518\n",
      "epoch 80: test loss is 1.5687814950942993\n",
      "epoch 90: train loss is 1.0315225582856398\n",
      "epoch 90: test loss is 1.417425513267517\n",
      "epoch 100: train loss is 0.9269878818438604\n",
      "epoch 100: test loss is 1.2759052515029907\n",
      "epoch 110: train loss is 0.8593139189940232\n",
      "epoch 110: test loss is 1.1327539682388306\n",
      "epoch 120: train loss is 0.7887061009040246\n",
      "epoch 120: test loss is 1.011712670326233\n",
      "epoch 130: train loss is 0.719049334526062\n",
      "epoch 130: test loss is 0.9149796962738037\n",
      "epoch 140: train loss is 0.6753196120262146\n",
      "epoch 140: test loss is 0.8500320315361023\n",
      "epoch 150: train loss is 0.6437432399162879\n",
      "epoch 150: test loss is 0.8180472254753113\n",
      "epoch 160: train loss is 0.6137837652976696\n",
      "epoch 160: test loss is 0.7875920534133911\n",
      "epoch 170: train loss is 0.5977163337744199\n",
      "epoch 170: test loss is 0.7497035264968872\n",
      "epoch 180: train loss is 0.5817604638062991\n",
      "epoch 180: test loss is 0.7146531343460083\n",
      "epoch 190: train loss is 0.5651349792113671\n",
      "epoch 190: test loss is 0.6834065318107605\n",
      "epoch 200: train loss is 0.5545693819339459\n",
      "epoch 200: test loss is 0.6594495177268982\n",
      "epoch 210: train loss is 0.5361916308219616\n",
      "epoch 210: test loss is 0.6306840181350708\n",
      "epoch 220: train loss is 0.5270065871568826\n",
      "epoch 220: test loss is 0.6174808740615845\n",
      "epoch 230: train loss is 0.5184105451290424\n",
      "epoch 230: test loss is 0.6031898856163025\n",
      "epoch 240: train loss is 0.5029665025380942\n",
      "epoch 240: test loss is 0.5921194553375244\n",
      "epoch 250: train loss is 0.48520408226893497\n",
      "epoch 250: test loss is 0.5719526410102844\n",
      "epoch 260: train loss is 0.4698918095001808\n",
      "epoch 260: test loss is 0.5510803461074829\n",
      "epoch 270: train loss is 0.44676409088648283\n",
      "epoch 270: test loss is 0.5235443115234375\n",
      "epoch 280: train loss is 0.42392950562330395\n",
      "epoch 280: test loss is 0.4901289641857147\n",
      "epoch 290: train loss is 0.40352362852830154\n",
      "epoch 290: test loss is 0.48668402433395386\n",
      "epoch 300: train loss is 0.3809107977610368\n",
      "epoch 300: test loss is 0.4300333559513092\n",
      "epoch 310: train loss is 0.3689965754747391\n",
      "epoch 310: test loss is 0.4187910854816437\n",
      "epoch 320: train loss is 0.3532857963672051\n",
      "epoch 320: test loss is 0.39596471190452576\n",
      "epoch 330: train loss is 0.3386586159467697\n",
      "epoch 330: test loss is 0.39036110043525696\n",
      "epoch 340: train loss is 0.3217699138017801\n",
      "epoch 340: test loss is 0.37848031520843506\n",
      "epoch 350: train loss is 0.3076609109456723\n",
      "epoch 350: test loss is 0.35076385736465454\n",
      "epoch 360: train loss is 0.29521811237702\n",
      "epoch 360: test loss is 0.3400445878505707\n",
      "epoch 370: train loss is 0.28862664905878216\n",
      "epoch 370: test loss is 0.34703564643859863\n",
      "epoch 380: train loss is 0.2671464360677279\n",
      "epoch 380: test loss is 0.3246355652809143\n",
      "epoch 390: train loss is 0.2572759917149177\n",
      "epoch 390: test loss is 0.3207879066467285\n",
      "epoch 400: train loss is 0.24726010744388288\n",
      "epoch 400: test loss is 0.30619490146636963\n",
      "epoch 410: train loss is 0.23647517768236306\n",
      "epoch 410: test loss is 0.2983855605125427\n",
      "epoch 420: train loss is 0.22656835844883552\n",
      "epoch 420: test loss is 0.27725979685783386\n",
      "epoch 430: train loss is 0.21756831728495085\n",
      "epoch 430: test loss is 0.2759511172771454\n",
      "epoch 440: train loss is 0.2131916622702892\n",
      "epoch 440: test loss is 0.24339859187602997\n",
      "epoch 450: train loss is 0.20190989799224413\n",
      "epoch 450: test loss is 0.23380550742149353\n",
      "epoch 460: train loss is 0.19635963611877882\n",
      "epoch 460: test loss is 0.22134944796562195\n",
      "epoch 470: train loss is 0.19237160568053907\n",
      "epoch 470: test loss is 0.21027660369873047\n",
      "epoch 480: train loss is 0.18657078192784235\n",
      "epoch 480: test loss is 0.246189147233963\n",
      "epoch 490: train loss is 0.18040492901435265\n",
      "epoch 490: test loss is 0.1985911726951599\n",
      "epoch 500: train loss is 0.17315486130806115\n",
      "epoch 500: test loss is 0.18483960628509521\n",
      "epoch 510: train loss is 0.17278224516373414\n",
      "epoch 510: test loss is 0.18667550384998322\n",
      "epoch 520: train loss is 0.16884491592645645\n",
      "epoch 520: test loss is 0.17986170947551727\n",
      "epoch 530: train loss is 0.16445751545520929\n",
      "epoch 530: test loss is 0.20233936607837677\n",
      "epoch 540: train loss is 0.16264630739505476\n",
      "epoch 540: test loss is 0.17291834950447083\n",
      "epoch 550: train loss is 0.1526902306538362\n",
      "epoch 550: test loss is 0.1777140200138092\n",
      "epoch 560: train loss is 0.15262498190769783\n",
      "epoch 560: test loss is 0.17202001810073853\n",
      "epoch 570: train loss is 0.1472112347300236\n",
      "epoch 570: test loss is 0.16827206313610077\n",
      "epoch 580: train loss is 0.1473471401975705\n",
      "epoch 580: test loss is 0.15668891370296478\n",
      "epoch 590: train loss is 0.14366037398576736\n",
      "epoch 590: test loss is 0.15250949561595917\n",
      "epoch 600: train loss is 0.14334213045927194\n",
      "epoch 600: test loss is 0.15353353321552277\n",
      "epoch 610: train loss is 0.13459771699630296\n",
      "epoch 610: test loss is 0.14572779834270477\n",
      "epoch 620: train loss is 0.13560215211831605\n",
      "epoch 620: test loss is 0.1596066653728485\n",
      "epoch 630: train loss is 0.1301685978586857\n",
      "epoch 630: test loss is 0.13627612590789795\n",
      "epoch 640: train loss is 0.13169852357644302\n",
      "epoch 640: test loss is 0.1512001007795334\n",
      "epoch 650: train loss is 0.12900131998153833\n",
      "epoch 650: test loss is 0.1380573958158493\n",
      "epoch 660: train loss is 0.12587589770555496\n",
      "epoch 660: test loss is 0.13058674335479736\n",
      "epoch 670: train loss is 0.12494309303852227\n",
      "epoch 670: test loss is 0.13027764856815338\n",
      "epoch 680: train loss is 0.12344538191190133\n",
      "epoch 680: test loss is 0.14434030652046204\n",
      "epoch 690: train loss is 0.12187721179081844\n",
      "epoch 690: test loss is 0.12702126801013947\n",
      "epoch 700: train loss is 0.12176905744350873\n",
      "epoch 700: test loss is 0.12801553308963776\n",
      "epoch 710: train loss is 0.12124087661504745\n",
      "epoch 710: test loss is 0.16442497074604034\n",
      "epoch 720: train loss is 0.11400792575799502\n",
      "epoch 720: test loss is 0.13302555680274963\n",
      "epoch 730: train loss is 0.11354719045070502\n",
      "epoch 730: test loss is 0.12617503106594086\n",
      "epoch 740: train loss is 0.11166084844332475\n",
      "epoch 740: test loss is 0.1239246055483818\n",
      "epoch 750: train loss is 0.11119670707445878\n",
      "epoch 750: test loss is 0.12048951536417007\n",
      "epoch 760: train loss is 0.11197982040735391\n",
      "epoch 760: test loss is 0.1650255024433136\n",
      "epoch 770: train loss is 0.10834563294282326\n",
      "epoch 770: test loss is 0.11693442612886429\n",
      "epoch 780: train loss is 0.10616609969964394\n",
      "epoch 780: test loss is 0.12456054240465164\n",
      "epoch 790: train loss is 0.10832068438713367\n",
      "epoch 790: test loss is 0.11767749488353729\n",
      "epoch 800: train loss is 0.10877817468001293\n",
      "epoch 800: test loss is 0.11511196196079254\n",
      "epoch 810: train loss is 0.10300185359441318\n",
      "epoch 810: test loss is 0.11604408919811249\n",
      "epoch 820: train loss is 0.10312068233123192\n",
      "epoch 820: test loss is 0.11216534674167633\n",
      "epoch 830: train loss is 0.10470544202969624\n",
      "epoch 830: test loss is 0.12251443415880203\n",
      "epoch 840: train loss is 0.10050695905318627\n",
      "epoch 840: test loss is 0.10942313075065613\n",
      "epoch 850: train loss is 0.09923711648354164\n",
      "epoch 850: test loss is 0.10749329626560211\n",
      "epoch 860: train loss is 0.09761905555541699\n",
      "epoch 860: test loss is 0.11727533489465714\n",
      "epoch 870: train loss is 0.11071826402957623\n",
      "epoch 870: test loss is 0.14124777913093567\n",
      "epoch 880: train loss is 0.09496379414430031\n",
      "epoch 880: test loss is 0.1068105399608612\n",
      "epoch 890: train loss is 0.09576132549689366\n",
      "epoch 890: test loss is 0.1139293983578682\n",
      "epoch 900: train loss is 0.09655357094911429\n",
      "epoch 900: test loss is 0.1124347671866417\n",
      "epoch 910: train loss is 0.09628900312460385\n",
      "epoch 910: test loss is 0.11762814223766327\n",
      "epoch 920: train loss is 0.09307507196298012\n",
      "epoch 920: test loss is 0.10236368328332901\n",
      "epoch 930: train loss is 0.09497488863193072\n",
      "epoch 930: test loss is 0.10710509121417999\n",
      "epoch 940: train loss is 0.0890077008650853\n",
      "epoch 940: test loss is 0.10504100471735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 950: train loss is 0.0936814947770192\n",
      "epoch 950: test loss is 0.11043991893529892\n",
      "epoch 960: train loss is 0.08907350897789001\n",
      "epoch 960: test loss is 0.10694782435894012\n",
      "epoch 970: train loss is 0.08891812425393325\n",
      "epoch 970: test loss is 0.12049774080514908\n",
      "epoch 980: train loss is 0.08568692207336426\n",
      "epoch 980: test loss is 0.10451760143041611\n",
      "epoch 990: train loss is 0.08822625474287914\n",
      "epoch 990: test loss is 0.10292647778987885\n",
      "epoch 1000: train loss is 0.08429510146379471\n",
      "epoch 1000: test loss is 0.0945446640253067\n",
      "epoch 1010: train loss is 0.08432925091339992\n",
      "epoch 1010: test loss is 0.09449945390224457\n",
      "epoch 1020: train loss is 0.08278253244665953\n",
      "epoch 1020: test loss is 0.09221123158931732\n",
      "epoch 1030: train loss is 0.08706506112447152\n",
      "epoch 1030: test loss is 0.09235889464616776\n",
      "epoch 1040: train loss is 0.08544111137206738\n",
      "epoch 1040: test loss is 0.10646640509366989\n",
      "epoch 1050: train loss is 0.08245864854409145\n",
      "epoch 1050: test loss is 0.0891491025686264\n",
      "epoch 1060: train loss is 0.08633281548435871\n",
      "epoch 1060: test loss is 0.09530142694711685\n",
      "epoch 1070: train loss is 0.07955478303707562\n",
      "epoch 1070: test loss is 0.08743106573820114\n",
      "epoch 1080: train loss is 0.08173741715458724\n",
      "epoch 1080: test loss is 0.08698610216379166\n",
      "epoch 1090: train loss is 0.08088951128033492\n",
      "epoch 1090: test loss is 0.08758853375911713\n",
      "epoch 1100: train loss is 0.07715872445931801\n",
      "epoch 1100: test loss is 0.08787614852190018\n",
      "epoch 1110: train loss is 0.08347582186643894\n",
      "epoch 1110: test loss is 0.12330872565507889\n",
      "epoch 1120: train loss is 0.08101779050551929\n",
      "epoch 1120: test loss is 0.08746672421693802\n",
      "epoch 1130: train loss is 0.07852431186116658\n",
      "epoch 1130: test loss is 0.08849717676639557\n",
      "epoch 1140: train loss is 0.08615372673823284\n",
      "epoch 1140: test loss is 0.08804932981729507\n",
      "epoch 1150: train loss is 0.07885386995398082\n",
      "epoch 1150: test loss is 0.10069458931684494\n",
      "epoch 1160: train loss is 0.08017850437989602\n",
      "epoch 1160: test loss is 0.11407575756311417\n",
      "epoch 1170: train loss is 0.07960411476401183\n",
      "epoch 1170: test loss is 0.08969927579164505\n",
      "epoch 1180: train loss is 0.07936070687495746\n",
      "epoch 1180: test loss is 0.08265283703804016\n",
      "epoch 1190: train loss is 0.07624484340731914\n",
      "epoch 1190: test loss is 0.08697865158319473\n",
      "epoch 1200: train loss is 0.08130732522561\n",
      "epoch 1200: test loss is 0.08276268094778061\n",
      "epoch 1210: train loss is 0.07311032884396039\n",
      "epoch 1210: test loss is 0.08352171629667282\n",
      "epoch 1220: train loss is 0.0741906796510403\n",
      "epoch 1220: test loss is 0.08412523567676544\n",
      "epoch 1230: train loss is 0.07435519706744415\n",
      "epoch 1230: test loss is 0.08680685609579086\n",
      "epoch 1240: train loss is 0.07273956388235092\n",
      "epoch 1240: test loss is 0.0976063460111618\n",
      "epoch 1250: train loss is 0.0725721364411024\n",
      "epoch 1250: test loss is 0.08613819628953934\n",
      "epoch 1260: train loss is 0.07684621014274083\n",
      "epoch 1260: test loss is 0.0870509073138237\n",
      "epoch 1270: train loss is 0.07743150253708546\n",
      "epoch 1270: test loss is 0.10500083118677139\n",
      "epoch 1280: train loss is 0.07656588376714633\n",
      "epoch 1280: test loss is 0.09112094342708588\n",
      "epoch 1290: train loss is 0.07313952021873914\n",
      "epoch 1290: test loss is 0.08065101504325867\n",
      "epoch 1300: train loss is 0.0700143976853444\n",
      "epoch 1300: test loss is 0.08075183629989624\n",
      "epoch 1310: train loss is 0.06926174719746296\n",
      "epoch 1310: test loss is 0.08069758862257004\n",
      "epoch 1320: train loss is 0.07228196297700588\n",
      "epoch 1320: test loss is 0.08061734586954117\n",
      "epoch 1330: train loss is 0.07286307903436515\n",
      "epoch 1330: test loss is 0.08532042056322098\n",
      "epoch 1340: train loss is 0.06763568262641247\n",
      "epoch 1340: test loss is 0.07874244451522827\n",
      "epoch 1350: train loss is 0.07636385009838985\n",
      "epoch 1350: test loss is 0.08392591774463654\n",
      "epoch 1360: train loss is 0.06786029986464061\n",
      "epoch 1360: test loss is 0.07351776212453842\n",
      "epoch 1370: train loss is 0.06515741978700344\n",
      "epoch 1370: test loss is 0.07327797263860703\n",
      "epoch 1380: train loss is 0.06788872554898262\n",
      "epoch 1380: test loss is 0.07934542745351791\n",
      "epoch 1390: train loss is 0.06875896052672313\n",
      "epoch 1390: test loss is 0.07724613696336746\n",
      "epoch 1400: train loss is 0.0720217855503926\n",
      "epoch 1400: test loss is 0.07803104817867279\n",
      "epoch 1410: train loss is 0.06603198211926681\n",
      "epoch 1410: test loss is 0.07759906351566315\n",
      "epoch 1420: train loss is 0.06781312479422642\n",
      "epoch 1420: test loss is 0.07779615372419357\n",
      "epoch 1430: train loss is 0.06636247382714199\n",
      "epoch 1430: test loss is 0.07156146317720413\n",
      "epoch 1440: train loss is 0.06764212651894642\n",
      "epoch 1440: test loss is 0.07306622713804245\n",
      "epoch 1450: train loss is 0.0663082221379647\n",
      "epoch 1450: test loss is 0.07720708101987839\n",
      "epoch 1460: train loss is 0.06427584044062175\n",
      "epoch 1460: test loss is 0.07201337814331055\n",
      "epoch 1470: train loss is 0.07172365343341461\n",
      "epoch 1470: test loss is 0.08126990497112274\n",
      "epoch 1480: train loss is 0.06489999649616388\n",
      "epoch 1480: test loss is 0.07145935297012329\n",
      "epoch 1490: train loss is 0.06532995460125116\n",
      "epoch 1490: test loss is 0.07493593543767929\n",
      "epoch 1500: train loss is 0.06714290122573192\n",
      "epoch 1500: test loss is 0.08484042435884476\n",
      "epoch 1510: train loss is 0.06441786856605457\n",
      "epoch 1510: test loss is 0.0758943185210228\n",
      "epoch 1520: train loss is 0.06720496398898271\n",
      "epoch 1520: test loss is 0.07994363456964493\n",
      "epoch 1530: train loss is 0.061811866955115244\n",
      "epoch 1530: test loss is 0.07663395255804062\n",
      "epoch 1540: train loss is 0.06384402857376979\n",
      "epoch 1540: test loss is 0.07904423773288727\n",
      "epoch 1550: train loss is 0.06543559151200148\n",
      "epoch 1550: test loss is 0.08034047484397888\n",
      "epoch 1560: train loss is 0.06002744545157139\n",
      "epoch 1560: test loss is 0.0680072084069252\n",
      "epoch 1570: train loss is 0.06019766628742218\n",
      "epoch 1570: test loss is 0.08138900995254517\n",
      "epoch 1580: train loss is 0.060373637825250626\n",
      "epoch 1580: test loss is 0.07464516907930374\n",
      "epoch 1590: train loss is 0.06132587647208801\n",
      "epoch 1590: test loss is 0.07597417384386063\n",
      "epoch 1600: train loss is 0.0678251379957566\n",
      "epoch 1600: test loss is 0.09575124830007553\n",
      "epoch 1610: train loss is 0.06351054660402812\n",
      "epoch 1610: test loss is 0.07669970393180847\n",
      "epoch 1620: train loss is 0.05986154767183157\n",
      "epoch 1620: test loss is 0.06654684990644455\n",
      "epoch 1630: train loss is 0.06441214995888564\n",
      "epoch 1630: test loss is 0.07376298308372498\n",
      "epoch 1640: train loss is 0.061187106829423174\n",
      "epoch 1640: test loss is 0.07176800817251205\n",
      "epoch 1650: train loss is 0.06298017415862817\n",
      "epoch 1650: test loss is 0.07069516181945801\n",
      "epoch 1660: train loss is 0.05950905411289288\n",
      "epoch 1660: test loss is 0.08380585163831711\n",
      "epoch 1670: train loss is 0.05933245185476083\n",
      "epoch 1670: test loss is 0.07233501970767975\n",
      "epoch 1680: train loss is 0.060590605896252855\n",
      "epoch 1680: test loss is 0.0690109133720398\n",
      "epoch 1690: train loss is 0.0717558218882634\n",
      "epoch 1690: test loss is 0.10436023771762848\n",
      "epoch 1700: train loss is 0.06289292241518314\n",
      "epoch 1700: test loss is 0.06530939042568207\n",
      "epoch 1710: train loss is 0.06250227300020364\n",
      "epoch 1710: test loss is 0.07361806184053421\n",
      "epoch 1720: train loss is 0.05716327950358391\n",
      "epoch 1720: test loss is 0.06654979288578033\n",
      "epoch 1730: train loss is 0.05820914759085728\n",
      "epoch 1730: test loss is 0.07123380154371262\n",
      "epoch 1740: train loss is 0.05718551232264592\n",
      "epoch 1740: test loss is 0.06679138541221619\n",
      "epoch 1750: train loss is 0.06170173820394736\n",
      "epoch 1750: test loss is 0.06639302521944046\n",
      "epoch 1760: train loss is 0.05929757253481792\n",
      "epoch 1760: test loss is 0.0664786770939827\n",
      "epoch 1770: train loss is 0.059063148040037886\n",
      "epoch 1770: test loss is 0.07342444360256195\n",
      "epoch 1780: train loss is 0.058891359430093035\n",
      "epoch 1780: test loss is 0.07885647565126419\n",
      "epoch 1790: train loss is 0.05753812852960367\n",
      "epoch 1790: test loss is 0.06725726276636124\n",
      "epoch 1800: train loss is 0.0573483737042317\n",
      "epoch 1800: test loss is 0.06302952766418457\n",
      "epoch 1810: train loss is 0.060135154196849235\n",
      "epoch 1810: test loss is 0.08736389875411987\n",
      "epoch 1820: train loss is 0.05710216124470417\n",
      "epoch 1820: test loss is 0.06336377561092377\n",
      "epoch 1830: train loss is 0.05768447226056686\n",
      "epoch 1830: test loss is 0.09641390293836594\n",
      "epoch 1840: train loss is 0.05671935786421482\n",
      "epoch 1840: test loss is 0.06834971159696579\n",
      "epoch 1850: train loss is 0.06010397466329428\n",
      "epoch 1850: test loss is 0.07119618356227875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1860: train loss is 0.05676528668174377\n",
      "epoch 1860: test loss is 0.06539056450128555\n",
      "epoch 1870: train loss is 0.05679202452301979\n",
      "epoch 1870: test loss is 0.08015908300876617\n",
      "epoch 1880: train loss is 0.05811929960663502\n",
      "epoch 1880: test loss is 0.06866083294153214\n",
      "epoch 1890: train loss is 0.0635170117020607\n",
      "epoch 1890: test loss is 0.07584673166275024\n",
      "epoch 1900: train loss is 0.05416517074291523\n",
      "epoch 1900: test loss is 0.07192743569612503\n",
      "epoch 1910: train loss is 0.05864936313950098\n",
      "epoch 1910: test loss is 0.061775147914886475\n",
      "epoch 1920: train loss is 0.05570981651544571\n",
      "epoch 1920: test loss is 0.061718087643384933\n",
      "epoch 1930: train loss is 0.05753259217509857\n",
      "epoch 1930: test loss is 0.06973929703235626\n",
      "epoch 1940: train loss is 0.056892263488127634\n",
      "epoch 1940: test loss is 0.06385049223899841\n",
      "epoch 1950: train loss is 0.0598383961388698\n",
      "epoch 1950: test loss is 0.0877169594168663\n",
      "epoch 1960: train loss is 0.055102588752141364\n",
      "epoch 1960: test loss is 0.060095518827438354\n",
      "epoch 1970: train loss is 0.059867265132757336\n",
      "epoch 1970: test loss is 0.10451164841651917\n",
      "epoch 1980: train loss is 0.0554776624418222\n",
      "epoch 1980: test loss is 0.07276684790849686\n",
      "epoch 1990: train loss is 0.058585331990168646\n",
      "epoch 1990: test loss is 0.06780949980020523\n",
      "epoch 2000: train loss is 0.05413004784629895\n",
      "epoch 2000: test loss is 0.06674963235855103\n",
      "epoch 2010: train loss is 0.05661851196334912\n",
      "epoch 2010: test loss is 0.059506747871637344\n",
      "epoch 2020: train loss is 0.06098183158498544\n",
      "epoch 2020: test loss is 0.06412054598331451\n",
      "epoch 2030: train loss is 0.0550496933551935\n",
      "epoch 2030: test loss is 0.06369462609291077\n",
      "epoch 2040: train loss is 0.05629363999916957\n",
      "epoch 2040: test loss is 0.07136242091655731\n",
      "epoch 2050: train loss is 0.05286559290610827\n",
      "epoch 2050: test loss is 0.058029208332300186\n",
      "epoch 2060: train loss is 0.052480573837573714\n",
      "epoch 2060: test loss is 0.060389768332242966\n",
      "epoch 2070: train loss is 0.057524178750239886\n",
      "epoch 2070: test loss is 0.07277338951826096\n",
      "epoch 2080: train loss is 0.052756576297374874\n",
      "epoch 2080: test loss is 0.05956553295254707\n",
      "epoch 2090: train loss is 0.0543057586138065\n",
      "epoch 2090: test loss is 0.06376397609710693\n",
      "epoch 2100: train loss is 0.0555724693605533\n",
      "epoch 2100: test loss is 0.07369612902402878\n",
      "epoch 2110: train loss is 0.057231609924481466\n",
      "epoch 2110: test loss is 0.06950629502534866\n",
      "epoch 2120: train loss is 0.0529130998139198\n",
      "epoch 2120: test loss is 0.06503862142562866\n",
      "epoch 2130: train loss is 0.05008852940339308\n",
      "epoch 2130: test loss is 0.06083151698112488\n",
      "epoch 2140: train loss is 0.05437060875388292\n",
      "epoch 2140: test loss is 0.062278684228658676\n",
      "epoch 2150: train loss is 0.05690135892767172\n",
      "epoch 2150: test loss is 0.07081723213195801\n",
      "epoch 2160: train loss is 0.05507880449295044\n",
      "epoch 2160: test loss is 0.06415165960788727\n",
      "epoch 2170: train loss is 0.05517591965886263\n",
      "epoch 2170: test loss is 0.06276866048574448\n",
      "epoch 2180: train loss is 0.054804831743240356\n",
      "epoch 2180: test loss is 0.06490878015756607\n",
      "epoch 2190: train loss is 0.05175980065877621\n",
      "epoch 2190: test loss is 0.06609907746315002\n",
      "epoch 2200: train loss is 0.0534514097067026\n",
      "epoch 2200: test loss is 0.061220258474349976\n",
      "epoch 2210: train loss is 0.0528524205661737\n",
      "epoch 2210: test loss is 0.05956483259797096\n",
      "epoch 2220: train loss is 0.05319403656400167\n",
      "epoch 2220: test loss is 0.06011902913451195\n",
      "epoch 2230: train loss is 0.05005487226522886\n",
      "epoch 2230: test loss is 0.055236995220184326\n",
      "epoch 2240: train loss is 0.05091165693906637\n",
      "epoch 2240: test loss is 0.05831247940659523\n",
      "epoch 2250: train loss is 0.053922442289499134\n",
      "epoch 2250: test loss is 0.058660462498664856\n",
      "epoch 2260: train loss is 0.05394558092722526\n",
      "epoch 2260: test loss is 0.0569637194275856\n",
      "epoch 2270: train loss is 0.0560321773474033\n",
      "epoch 2270: test loss is 0.06137651577591896\n",
      "epoch 2280: train loss is 0.05031231704812784\n",
      "epoch 2280: test loss is 0.05826122686266899\n",
      "epoch 2290: train loss is 0.055730857814733796\n",
      "epoch 2290: test loss is 0.06171783059835434\n",
      "epoch 2300: train loss is 0.05045940641027231\n",
      "epoch 2300: test loss is 0.054189909249544144\n",
      "epoch 2310: train loss is 0.05119280115916179\n",
      "epoch 2310: test loss is 0.053772248327732086\n",
      "epoch 2320: train loss is 0.053677613918597884\n",
      "epoch 2320: test loss is 0.06501045823097229\n",
      "epoch 2330: train loss is 0.05267769155594019\n",
      "epoch 2330: test loss is 0.05830339342355728\n",
      "epoch 2340: train loss is 0.04982558494577041\n",
      "epoch 2340: test loss is 0.05695369839668274\n",
      "epoch 2350: train loss is 0.05132948463925949\n",
      "epoch 2350: test loss is 0.059419747442007065\n",
      "epoch 2360: train loss is 0.050668069089834504\n",
      "epoch 2360: test loss is 0.05511081963777542\n",
      "epoch 2370: train loss is 0.05271631565231543\n",
      "epoch 2370: test loss is 0.0648706927895546\n",
      "epoch 2380: train loss is 0.050155057643468566\n",
      "epoch 2380: test loss is 0.06455458700656891\n",
      "epoch 2390: train loss is 0.051104080505095996\n",
      "epoch 2390: test loss is 0.06536325067281723\n",
      "epoch 2400: train loss is 0.05107095293127573\n",
      "epoch 2400: test loss is 0.0545920692384243\n",
      "epoch 2410: train loss is 0.05211135716392444\n",
      "epoch 2410: test loss is 0.059766001999378204\n",
      "epoch 2420: train loss is 0.053033010890850656\n",
      "epoch 2420: test loss is 0.06704024225473404\n",
      "epoch 2430: train loss is 0.05632305145263672\n",
      "epoch 2430: test loss is 0.059424467384815216\n",
      "epoch 2440: train loss is 0.05060717291556872\n",
      "epoch 2440: test loss is 0.05665884539484978\n",
      "epoch 2450: train loss is 0.052729060443548054\n",
      "epoch 2450: test loss is 0.05428719520568848\n",
      "epoch 2460: train loss is 0.05041468172119214\n",
      "epoch 2460: test loss is 0.06762000173330307\n",
      "epoch 2470: train loss is 0.04927532030985905\n",
      "epoch 2470: test loss is 0.05768469721078873\n",
      "epoch 2480: train loss is 0.050499812341653384\n",
      "epoch 2480: test loss is 0.05602778121829033\n",
      "epoch 2490: train loss is 0.05056616463340246\n",
      "epoch 2490: test loss is 0.052632227540016174\n",
      "epoch 2500: train loss is 0.05283858856329551\n",
      "epoch 2500: test loss is 0.057637762278318405\n",
      "epoch 2510: train loss is 0.055078621953725815\n",
      "epoch 2510: test loss is 0.07390811294317245\n",
      "epoch 2520: train loss is 0.04959861676280315\n",
      "epoch 2520: test loss is 0.05560734122991562\n",
      "epoch 2530: train loss is 0.04715632962492796\n",
      "epoch 2530: test loss is 0.05400934815406799\n",
      "epoch 2540: train loss is 0.05304188711138872\n",
      "epoch 2540: test loss is 0.05630058795213699\n",
      "epoch 2550: train loss is 0.052279883852371804\n",
      "epoch 2550: test loss is 0.07434354722499847\n",
      "epoch 2560: train loss is 0.04864089592145039\n",
      "epoch 2560: test loss is 0.05789322778582573\n",
      "epoch 2570: train loss is 0.05036217776628641\n",
      "epoch 2570: test loss is 0.05359195917844772\n",
      "epoch 2580: train loss is 0.04994580235618811\n",
      "epoch 2580: test loss is 0.058060962706804276\n",
      "epoch 2590: train loss is 0.04879805187766369\n",
      "epoch 2590: test loss is 0.06050180271267891\n",
      "epoch 2600: train loss is 0.05021901858540682\n",
      "epoch 2600: test loss is 0.05428846925497055\n",
      "epoch 2610: train loss is 0.04792513755651621\n",
      "epoch 2610: test loss is 0.05138735845685005\n",
      "epoch 2620: train loss is 0.0490110287299523\n",
      "epoch 2620: test loss is 0.053670383989810944\n",
      "epoch 2630: train loss is 0.05072777087871845\n",
      "epoch 2630: test loss is 0.06405548751354218\n",
      "epoch 2640: train loss is 0.052106102785238854\n",
      "epoch 2640: test loss is 0.05654905363917351\n",
      "epoch 2650: train loss is 0.04992926923128275\n",
      "epoch 2650: test loss is 0.052706342190504074\n",
      "epoch 2660: train loss is 0.04767252877354622\n",
      "epoch 2660: test loss is 0.06001945957541466\n",
      "epoch 2670: train loss is 0.051432233017224535\n",
      "epoch 2670: test loss is 0.059111468493938446\n",
      "epoch 2680: train loss is 0.0471166896705444\n",
      "epoch 2680: test loss is 0.05954140052199364\n",
      "epoch 2690: train loss is 0.049709114604271375\n",
      "epoch 2690: test loss is 0.05405382066965103\n",
      "epoch 2700: train loss is 0.049241251383836455\n",
      "epoch 2700: test loss is 0.07044721394777298\n",
      "epoch 2710: train loss is 0.04782292934564444\n",
      "epoch 2710: test loss is 0.05178521201014519\n",
      "epoch 2720: train loss is 0.04949117566530521\n",
      "epoch 2720: test loss is 0.057434916496276855\n",
      "epoch 2730: train loss is 0.046739922979703315\n",
      "epoch 2730: test loss is 0.05159250646829605\n",
      "epoch 2740: train loss is 0.048956942959473684\n",
      "epoch 2740: test loss is 0.05080971494317055\n",
      "epoch 2750: train loss is 0.048706771376041263\n",
      "epoch 2750: test loss is 0.056523650884628296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2760: train loss is 0.04843809025791975\n",
      "epoch 2760: test loss is 0.059098392724990845\n",
      "epoch 2770: train loss is 0.04634076442856055\n",
      "epoch 2770: test loss is 0.04953935369849205\n",
      "epoch 2780: train loss is 0.049143391446425364\n",
      "epoch 2780: test loss is 0.055953498929739\n",
      "epoch 2790: train loss is 0.04716561992581074\n",
      "epoch 2790: test loss is 0.05273287370800972\n",
      "epoch 2800: train loss is 0.04715073080017017\n",
      "epoch 2800: test loss is 0.05701223388314247\n",
      "epoch 2810: train loss is 0.049512849834102854\n",
      "epoch 2810: test loss is 0.0538952574133873\n",
      "epoch 2820: train loss is 0.04589957944475687\n",
      "epoch 2820: test loss is 0.051361922174692154\n",
      "epoch 2830: train loss is 0.045699395812474765\n",
      "epoch 2830: test loss is 0.04883619025349617\n",
      "epoch 2840: train loss is 0.04501406189340811\n",
      "epoch 2840: test loss is 0.057778093963861465\n",
      "epoch 2850: train loss is 0.046245902203596555\n",
      "epoch 2850: test loss is 0.0522785447537899\n",
      "epoch 2860: train loss is 0.04650447660913834\n",
      "epoch 2860: test loss is 0.05191315710544586\n",
      "epoch 2870: train loss is 0.04659438133239746\n",
      "epoch 2870: test loss is 0.0510069914162159\n",
      "epoch 2880: train loss is 0.04626999422907829\n",
      "epoch 2880: test loss is 0.05402958020567894\n",
      "epoch 2890: train loss is 0.04537157571086517\n",
      "epoch 2890: test loss is 0.05197667330503464\n",
      "epoch 2900: train loss is 0.04657893799818479\n",
      "epoch 2900: test loss is 0.0489964634180069\n",
      "epoch 2910: train loss is 0.049743724843630426\n",
      "epoch 2910: test loss is 0.06610850989818573\n",
      "epoch 2920: train loss is 0.047933704004837915\n",
      "epoch 2920: test loss is 0.05422167479991913\n",
      "epoch 2930: train loss is 0.050349460484889835\n",
      "epoch 2930: test loss is 0.05623083934187889\n",
      "epoch 2940: train loss is 0.04708474043470163\n",
      "epoch 2940: test loss is 0.05834626406431198\n",
      "epoch 2950: train loss is 0.047461444368729226\n",
      "epoch 2950: test loss is 0.04804357886314392\n",
      "epoch 2960: train loss is 0.04695961423791372\n",
      "epoch 2960: test loss is 0.05480710044503212\n",
      "epoch 2970: train loss is 0.04797103055394613\n",
      "epoch 2970: test loss is 0.057263556867837906\n",
      "epoch 2980: train loss is 0.04724027388370954\n",
      "epoch 2980: test loss is 0.047436196357011795\n",
      "epoch 2990: train loss is 0.04547442295230352\n",
      "epoch 2990: test loss is 0.06078019365668297\n",
      "epoch 3000: train loss is 0.04929498181893276\n",
      "epoch 3000: test loss is 0.05281084030866623\n",
      "epoch 3010: train loss is 0.0490704643038603\n",
      "epoch 3010: test loss is 0.06835171580314636\n",
      "epoch 3020: train loss is 0.04427163274242328\n",
      "epoch 3020: test loss is 0.047412700951099396\n",
      "epoch 3030: train loss is 0.045066582182279\n",
      "epoch 3030: test loss is 0.050289034843444824\n",
      "epoch 3040: train loss is 0.04620150161477236\n",
      "epoch 3040: test loss is 0.05486854165792465\n",
      "epoch 3050: train loss is 0.046835300154410876\n",
      "epoch 3050: test loss is 0.0579020120203495\n",
      "epoch 3060: train loss is 0.04473415704873892\n",
      "epoch 3060: test loss is 0.04819824919104576\n",
      "epoch 3070: train loss is 0.04557723637956839\n",
      "epoch 3070: test loss is 0.04853229597210884\n",
      "epoch 3080: train loss is 0.046817949471565395\n",
      "epoch 3080: test loss is 0.04980587959289551\n",
      "epoch 3090: train loss is 0.044840869708703116\n",
      "epoch 3090: test loss is 0.048347730189561844\n",
      "epoch 3100: train loss is 0.04588085011794017\n",
      "epoch 3100: test loss is 0.057050470262765884\n",
      "epoch 3110: train loss is 0.04725572552818518\n",
      "epoch 3110: test loss is 0.06059301644563675\n",
      "epoch 3120: train loss is 0.044798666468033425\n",
      "epoch 3120: test loss is 0.05415062606334686\n",
      "epoch 3130: train loss is 0.0449663707270072\n",
      "epoch 3130: test loss is 0.05129629746079445\n",
      "epoch 3140: train loss is 0.04338185775738496\n",
      "epoch 3140: test loss is 0.046355951577425\n",
      "epoch 3150: train loss is 0.043690938215989336\n",
      "epoch 3150: test loss is 0.05454237014055252\n",
      "epoch 3160: train loss is 0.04682567429084044\n",
      "epoch 3160: test loss is 0.05066632479429245\n",
      "epoch 3170: train loss is 0.04528882870307335\n",
      "epoch 3170: test loss is 0.05536746606230736\n",
      "epoch 3180: train loss is 0.042772578505369335\n",
      "epoch 3180: test loss is 0.04877293482422829\n",
      "epoch 3190: train loss is 0.044831219487465344\n",
      "epoch 3190: test loss is 0.05151848495006561\n",
      "epoch 3200: train loss is 0.043296924577309534\n",
      "epoch 3200: test loss is 0.05101211741566658\n",
      "epoch 3210: train loss is 0.04532176055587255\n",
      "epoch 3210: test loss is 0.06219009682536125\n",
      "epoch 3220: train loss is 0.044910724059893534\n",
      "epoch 3220: test loss is 0.05507007986307144\n",
      "epoch 3230: train loss is 0.04635114595293999\n",
      "epoch 3230: test loss is 0.051898106932640076\n",
      "epoch 3240: train loss is 0.04560091031285433\n",
      "epoch 3240: test loss is 0.04872288927435875\n",
      "epoch 3250: train loss is 0.044942532307826556\n",
      "epoch 3250: test loss is 0.04863978922367096\n",
      "epoch 3260: train loss is 0.041726988095503584\n",
      "epoch 3260: test loss is 0.04693147540092468\n",
      "epoch 3270: train loss is 0.0450845191685053\n",
      "epoch 3270: test loss is 0.04878503829240799\n",
      "epoch 3280: train loss is 0.04657590847748976\n",
      "epoch 3280: test loss is 0.051595043390989304\n",
      "epoch 3290: train loss is 0.04483250643198307\n",
      "epoch 3290: test loss is 0.05025782436132431\n",
      "epoch 3300: train loss is 0.044653318249262296\n",
      "epoch 3300: test loss is 0.04948878288269043\n",
      "epoch 3310: train loss is 0.04470975295855449\n",
      "epoch 3310: test loss is 0.04737044870853424\n",
      "epoch 3320: train loss is 0.044893225511679284\n",
      "epoch 3320: test loss is 0.046071019023656845\n",
      "epoch 3330: train loss is 0.048505743249104574\n",
      "epoch 3330: test loss is 0.05022409185767174\n",
      "epoch 3340: train loss is 0.04379151876156147\n",
      "epoch 3340: test loss is 0.04571082442998886\n",
      "epoch 3350: train loss is 0.045218249066517904\n",
      "epoch 3350: test loss is 0.046235695481300354\n",
      "epoch 3360: train loss is 0.0460512933249657\n",
      "epoch 3360: test loss is 0.04503209888935089\n",
      "epoch 3370: train loss is 0.04626162206897369\n",
      "epoch 3370: test loss is 0.05341659113764763\n",
      "epoch 3380: train loss is 0.04177312466960687\n",
      "epoch 3380: test loss is 0.04543875902891159\n",
      "epoch 3390: train loss is 0.044986562946668036\n",
      "epoch 3390: test loss is 0.05244484916329384\n",
      "epoch 3400: train loss is 0.042997838786015145\n",
      "epoch 3400: test loss is 0.049753811210393906\n",
      "epoch 3410: train loss is 0.04359365254640579\n",
      "epoch 3410: test loss is 0.045845821499824524\n",
      "epoch 3420: train loss is 0.044057523975005515\n",
      "epoch 3420: test loss is 0.04876508563756943\n",
      "epoch 3430: train loss is 0.04603297865161529\n",
      "epoch 3430: test loss is 0.06748142093420029\n",
      "epoch 3440: train loss is 0.048644256133299604\n",
      "epoch 3440: test loss is 0.05373063310980797\n",
      "epoch 3450: train loss is 0.04598604744443527\n",
      "epoch 3450: test loss is 0.04921428859233856\n",
      "epoch 3460: train loss is 0.0434336057649209\n",
      "epoch 3460: test loss is 0.04751114919781685\n",
      "epoch 3470: train loss is 0.04517272028785486\n",
      "epoch 3470: test loss is 0.05270134657621384\n",
      "epoch 3480: train loss is 0.043369064250817664\n",
      "epoch 3480: test loss is 0.04792836308479309\n",
      "epoch 3490: train loss is 0.04440477748329823\n",
      "epoch 3490: test loss is 0.04780176654458046\n",
      "epoch 3500: train loss is 0.04427580420787518\n",
      "epoch 3500: test loss is 0.049555499106645584\n",
      "epoch 3510: train loss is 0.045874638053087086\n",
      "epoch 3510: test loss is 0.04825539514422417\n",
      "epoch 3520: train loss is 0.04382398839180286\n",
      "epoch 3520: test loss is 0.04622747376561165\n",
      "epoch 3530: train loss is 0.04518004334889925\n",
      "epoch 3530: test loss is 0.04972222447395325\n",
      "epoch 3540: train loss is 0.04643763916996809\n",
      "epoch 3540: test loss is 0.058257006108760834\n",
      "epoch 3550: train loss is 0.04305333744447965\n",
      "epoch 3550: test loss is 0.049648042768239975\n",
      "epoch 3560: train loss is 0.04241202442118755\n",
      "epoch 3560: test loss is 0.04511193558573723\n",
      "epoch 3570: train loss is 0.04341270287449543\n",
      "epoch 3570: test loss is 0.050854478031396866\n",
      "epoch 3580: train loss is 0.042428528173611715\n",
      "epoch 3580: test loss is 0.052057381719350815\n",
      "epoch 3590: train loss is 0.04607873180737862\n",
      "epoch 3590: test loss is 0.05154009535908699\n",
      "epoch 3600: train loss is 0.04253292427613185\n",
      "epoch 3600: test loss is 0.04843999445438385\n",
      "epoch 3610: train loss is 0.041606320498081356\n",
      "epoch 3610: test loss is 0.04450920969247818\n",
      "epoch 3620: train loss is 0.04979332450490732\n",
      "epoch 3620: test loss is 0.053235799074172974\n",
      "epoch 3630: train loss is 0.04466315989310925\n",
      "epoch 3630: test loss is 0.04977875575423241\n",
      "epoch 3640: train loss is 0.04093546706896562\n",
      "epoch 3640: test loss is 0.044119734317064285\n",
      "epoch 3650: train loss is 0.041675698871795945\n",
      "epoch 3650: test loss is 0.04925528168678284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3660: train loss is 0.041768234223127365\n",
      "epoch 3660: test loss is 0.04623996838927269\n",
      "epoch 3670: train loss is 0.04504135709542494\n",
      "epoch 3670: test loss is 0.04672356694936752\n",
      "epoch 3680: train loss is 0.04024764322317564\n",
      "epoch 3680: test loss is 0.04915256053209305\n",
      "epoch 3690: train loss is 0.04174447876329605\n",
      "epoch 3690: test loss is 0.04347578436136246\n",
      "epoch 3700: train loss is 0.04705931733434017\n",
      "epoch 3700: test loss is 0.05512363836169243\n",
      "epoch 3710: train loss is 0.04119631820000135\n",
      "epoch 3710: test loss is 0.04709240794181824\n",
      "epoch 3720: train loss is 0.04296022209410484\n",
      "epoch 3720: test loss is 0.04459883272647858\n",
      "epoch 3730: train loss is 0.0413961593921368\n",
      "epoch 3730: test loss is 0.04243180900812149\n",
      "epoch 3740: train loss is 0.043667793847047366\n",
      "epoch 3740: test loss is 0.05469634011387825\n",
      "epoch 3750: train loss is 0.03956540502034701\n",
      "epoch 3750: test loss is 0.04480160027742386\n",
      "epoch 3760: train loss is 0.04377989671551264\n",
      "epoch 3760: test loss is 0.043035075068473816\n",
      "epoch 3770: train loss is 0.0418561682678186\n",
      "epoch 3770: test loss is 0.04521067440509796\n",
      "epoch 3780: train loss is 0.04134729418617029\n",
      "epoch 3780: test loss is 0.04327041655778885\n",
      "epoch 3790: train loss is 0.04032779929156487\n",
      "epoch 3790: test loss is 0.047922052443027496\n",
      "epoch 3800: train loss is 0.042973109449331574\n",
      "epoch 3800: test loss is 0.05382200703024864\n",
      "epoch 3810: train loss is 0.04467794127189196\n",
      "epoch 3810: test loss is 0.04825315624475479\n",
      "epoch 3820: train loss is 0.041501643565984875\n",
      "epoch 3820: test loss is 0.044319070875644684\n",
      "epoch 3830: train loss is 0.04337198602465483\n",
      "epoch 3830: test loss is 0.04834524169564247\n",
      "epoch 3840: train loss is 0.04120189037460547\n",
      "epoch 3840: test loss is 0.04846229404211044\n",
      "epoch 3850: train loss is 0.04346021322103647\n",
      "epoch 3850: test loss is 0.04991498589515686\n",
      "epoch 3860: train loss is 0.045157282828138426\n",
      "epoch 3860: test loss is 0.06364800781011581\n",
      "epoch 3870: train loss is 0.04469854527941117\n",
      "epoch 3870: test loss is 0.04736766219139099\n",
      "epoch 3880: train loss is 0.04020379985181185\n",
      "epoch 3880: test loss is 0.04488192871212959\n",
      "epoch 3890: train loss is 0.041132706575668775\n",
      "epoch 3890: test loss is 0.0469249002635479\n",
      "epoch 3900: train loss is 0.04162522376729892\n",
      "epoch 3900: test loss is 0.04920673742890358\n",
      "epoch 3910: train loss is 0.04125190211030153\n",
      "epoch 3910: test loss is 0.0461973212659359\n",
      "epoch 3920: train loss is 0.041486047638150364\n",
      "epoch 3920: test loss is 0.04638000950217247\n",
      "epoch 3930: train loss is 0.04238049084177384\n",
      "epoch 3930: test loss is 0.04994913563132286\n",
      "epoch 3940: train loss is 0.044213711642302\n",
      "epoch 3940: test loss is 0.04259645193815231\n",
      "epoch 3950: train loss is 0.04110712443406765\n",
      "epoch 3950: test loss is 0.05215168744325638\n",
      "epoch 3960: train loss is 0.039025384646195635\n",
      "epoch 3960: test loss is 0.043287210166454315\n",
      "epoch 3970: train loss is 0.0405891163704487\n",
      "epoch 3970: test loss is 0.04842276871204376\n",
      "epoch 3980: train loss is 0.041166662596739255\n",
      "epoch 3980: test loss is 0.04275903105735779\n",
      "epoch 3990: train loss is 0.03981925962636104\n",
      "epoch 3990: test loss is 0.0453963465988636\n",
      "epoch 4000: train loss is 0.04153707680793909\n",
      "epoch 4000: test loss is 0.04884352907538414\n",
      "epoch 4010: train loss is 0.04694605647371365\n",
      "epoch 4010: test loss is 0.06053908169269562\n",
      "epoch 4020: train loss is 0.04184069541784433\n",
      "epoch 4020: test loss is 0.044998910278081894\n",
      "epoch 4030: train loss is 0.041719114837738186\n",
      "epoch 4030: test loss is 0.04919655621051788\n",
      "epoch 4040: train loss is 0.04212788721689811\n",
      "epoch 4040: test loss is 0.04738835245370865\n",
      "epoch 4050: train loss is 0.04029650585009502\n",
      "epoch 4050: test loss is 0.04938340559601784\n",
      "epoch 4060: train loss is 0.04076161436163462\n",
      "epoch 4060: test loss is 0.04576292634010315\n",
      "epoch 4070: train loss is 0.039682167797134474\n",
      "epoch 4070: test loss is 0.042775899171829224\n",
      "epoch 4080: train loss is 0.044457463977428585\n",
      "epoch 4080: test loss is 0.053749650716781616\n",
      "epoch 4090: train loss is 0.04239020238702114\n",
      "epoch 4090: test loss is 0.04503888264298439\n",
      "epoch 4100: train loss is 0.04229522324525393\n",
      "epoch 4100: test loss is 0.043576788157224655\n",
      "epoch 4110: train loss is 0.041000691457436636\n",
      "epoch 4110: test loss is 0.04512455686926842\n",
      "epoch 4120: train loss is 0.046581748013313\n",
      "epoch 4120: test loss is 0.04546648636460304\n",
      "epoch 4130: train loss is 0.041682516439602926\n",
      "epoch 4130: test loss is 0.045201703906059265\n",
      "epoch 4140: train loss is 0.041073766751931265\n",
      "epoch 4140: test loss is 0.051104284822940826\n",
      "epoch 4150: train loss is 0.040126585616515234\n",
      "epoch 4150: test loss is 0.04715195670723915\n",
      "epoch 4160: train loss is 0.04315346192855101\n",
      "epoch 4160: test loss is 0.04255589470267296\n",
      "epoch 4170: train loss is 0.043071361688467175\n",
      "epoch 4170: test loss is 0.047503307461738586\n",
      "epoch 4180: train loss is 0.04126026123189009\n",
      "epoch 4180: test loss is 0.04371582344174385\n",
      "epoch 4190: train loss is 0.04185880677631268\n",
      "epoch 4190: test loss is 0.04482773691415787\n",
      "epoch 4200: train loss is 0.04203540144058374\n",
      "epoch 4200: test loss is 0.0671629011631012\n",
      "epoch 4210: train loss is 0.04173361997191723\n",
      "epoch 4210: test loss is 0.04945329576730728\n",
      "epoch 4220: train loss is 0.04015932891231317\n",
      "epoch 4220: test loss is 0.05100978910923004\n",
      "epoch 4230: train loss is 0.03958250338641497\n",
      "epoch 4230: test loss is 0.04412168264389038\n",
      "epoch 4240: train loss is 0.04185978452173563\n",
      "epoch 4240: test loss is 0.042935632169246674\n",
      "epoch 4250: train loss is 0.04082893164685139\n",
      "epoch 4250: test loss is 0.05997589975595474\n",
      "epoch 4260: train loss is 0.04378792385642345\n",
      "epoch 4260: test loss is 0.04183756560087204\n",
      "epoch 4270: train loss is 0.043030145936287366\n",
      "epoch 4270: test loss is 0.041611284017562866\n",
      "epoch 4280: train loss is 0.03848188972243896\n",
      "epoch 4280: test loss is 0.04451519250869751\n",
      "epoch 4290: train loss is 0.03866059304429935\n",
      "epoch 4290: test loss is 0.05304747447371483\n",
      "epoch 4300: train loss is 0.04050787423665707\n",
      "epoch 4300: test loss is 0.04411312937736511\n",
      "epoch 4310: train loss is 0.03892267824938664\n",
      "epoch 4310: test loss is 0.043880727142095566\n",
      "epoch 4320: train loss is 0.04106888662164028\n",
      "epoch 4320: test loss is 0.04472658783197403\n",
      "epoch 4330: train loss is 0.04197736800863193\n",
      "epoch 4330: test loss is 0.0483459010720253\n",
      "epoch 4340: train loss is 0.03799318078045662\n",
      "epoch 4340: test loss is 0.04190042242407799\n",
      "epoch 4350: train loss is 0.040363687018935494\n",
      "epoch 4350: test loss is 0.045134734362363815\n",
      "epoch 4360: train loss is 0.042637087691288725\n",
      "epoch 4360: test loss is 0.042900703847408295\n",
      "epoch 4370: train loss is 0.04320605013232965\n",
      "epoch 4370: test loss is 0.045556627213954926\n",
      "epoch 4380: train loss is 0.03750034006169209\n",
      "epoch 4380: test loss is 0.042780932039022446\n",
      "epoch 4390: train loss is 0.04159863321827008\n",
      "epoch 4390: test loss is 0.04345918446779251\n",
      "epoch 4400: train loss is 0.03904924942896916\n",
      "epoch 4400: test loss is 0.045367538928985596\n",
      "epoch 4410: train loss is 0.04200978118639726\n",
      "epoch 4410: test loss is 0.04397815838456154\n",
      "epoch 4420: train loss is 0.041099876165390015\n",
      "epoch 4420: test loss is 0.044958751648664474\n",
      "epoch 4430: train loss is 0.039277346374896854\n",
      "epoch 4430: test loss is 0.044462256133556366\n",
      "epoch 4440: train loss is 0.04372524498746945\n",
      "epoch 4440: test loss is 0.0432543121278286\n",
      "epoch 4450: train loss is 0.04372546907800894\n",
      "epoch 4450: test loss is 0.045982204377651215\n",
      "epoch 4460: train loss is 0.04106758592220453\n",
      "epoch 4460: test loss is 0.04407284036278725\n",
      "epoch 4470: train loss is 0.038542454345868185\n",
      "epoch 4470: test loss is 0.04570150002837181\n",
      "epoch 4480: train loss is 0.04186181566463067\n",
      "epoch 4480: test loss is 0.0462920181453228\n",
      "epoch 4490: train loss is 0.039007123273152575\n",
      "epoch 4490: test loss is 0.04086139798164368\n",
      "epoch 4500: train loss is 0.04096460972840969\n",
      "epoch 4500: test loss is 0.048010896891355515\n",
      "epoch 4510: train loss is 0.040267661500435606\n",
      "epoch 4510: test loss is 0.045894935727119446\n",
      "epoch 4520: train loss is 0.03837191471113609\n",
      "epoch 4520: test loss is 0.04089369252324104\n",
      "epoch 4530: train loss is 0.04063006862998009\n",
      "epoch 4530: test loss is 0.04081474244594574\n",
      "epoch 4540: train loss is 0.04018319512789066\n",
      "epoch 4540: test loss is 0.041617587208747864\n",
      "epoch 4550: train loss is 0.04022364347026898\n",
      "epoch 4550: test loss is 0.043377116322517395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4560: train loss is 0.04201131944472973\n",
      "epoch 4560: test loss is 0.045710012316703796\n",
      "epoch 4570: train loss is 0.03961897927981157\n",
      "epoch 4570: test loss is 0.046965938061475754\n",
      "epoch 4580: train loss is 0.039009280216235384\n",
      "epoch 4580: test loss is 0.042621880769729614\n",
      "epoch 4590: train loss is 0.04268595948815346\n",
      "epoch 4590: test loss is 0.05164588987827301\n",
      "epoch 4600: train loss is 0.0395154500236878\n",
      "epoch 4600: test loss is 0.04314814880490303\n",
      "epoch 4610: train loss is 0.03924050869850012\n",
      "epoch 4610: test loss is 0.04503166303038597\n",
      "epoch 4620: train loss is 0.039343720445266135\n",
      "epoch 4620: test loss is 0.06853953748941422\n",
      "epoch 4630: train loss is 0.040209803730249405\n",
      "epoch 4630: test loss is 0.0400882363319397\n",
      "epoch 4640: train loss is 0.040525804918545946\n",
      "epoch 4640: test loss is 0.05454649403691292\n",
      "epoch 4650: train loss is 0.04369250427071865\n",
      "epoch 4650: test loss is 0.05621229484677315\n",
      "epoch 4660: train loss is 0.040242843329906464\n",
      "epoch 4660: test loss is 0.04108278080821037\n",
      "epoch 4670: train loss is 0.036964381400209204\n",
      "epoch 4670: test loss is 0.04073752090334892\n",
      "epoch 4680: train loss is 0.038495833484026104\n",
      "epoch 4680: test loss is 0.05027832090854645\n",
      "epoch 4690: train loss is 0.03767916947030104\n",
      "epoch 4690: test loss is 0.04227118566632271\n",
      "epoch 4700: train loss is 0.03792720187741976\n",
      "epoch 4700: test loss is 0.04165274277329445\n",
      "epoch 4710: train loss is 0.0389531386586336\n",
      "epoch 4710: test loss is 0.054572515189647675\n",
      "epoch 4720: train loss is 0.040141391926086865\n",
      "epoch 4720: test loss is 0.042218029499053955\n",
      "epoch 4730: train loss is 0.03874486928375868\n",
      "epoch 4730: test loss is 0.041465699672698975\n",
      "epoch 4740: train loss is 0.03877647307056647\n",
      "epoch 4740: test loss is 0.04850558564066887\n",
      "epoch 4750: train loss is 0.040768011258198664\n",
      "epoch 4750: test loss is 0.043517373502254486\n",
      "epoch 4760: train loss is 0.044748335503614865\n",
      "epoch 4760: test loss is 0.04541598632931709\n",
      "epoch 4770: train loss is 0.036069368943572044\n",
      "epoch 4770: test loss is 0.04175904765725136\n",
      "epoch 4780: train loss is 0.036321922133748345\n",
      "epoch 4780: test loss is 0.0395926758646965\n",
      "epoch 4790: train loss is 0.04040549552211395\n",
      "epoch 4790: test loss is 0.048799414187669754\n",
      "epoch 4800: train loss is 0.03787858397341692\n",
      "epoch 4800: test loss is 0.041790127754211426\n",
      "epoch 4810: train loss is 0.04157065944029735\n",
      "epoch 4810: test loss is 0.045529626309871674\n",
      "epoch 4820: train loss is 0.04255030117928982\n",
      "epoch 4820: test loss is 0.046289388090372086\n",
      "epoch 4830: train loss is 0.039424350628486045\n",
      "epoch 4830: test loss is 0.04064173623919487\n",
      "epoch 4840: train loss is 0.037943266332149506\n",
      "epoch 4840: test loss is 0.0403701588511467\n",
      "epoch 4850: train loss is 0.03629205476206083\n",
      "epoch 4850: test loss is 0.046811480075120926\n",
      "epoch 4860: train loss is 0.037575535189646944\n",
      "epoch 4860: test loss is 0.04654967412352562\n",
      "epoch 4870: train loss is 0.03869351343466686\n",
      "epoch 4870: test loss is 0.05551539734005928\n",
      "epoch 4880: train loss is 0.036681322094339594\n",
      "epoch 4880: test loss is 0.04498932138085365\n",
      "epoch 4890: train loss is 0.03743545037622635\n",
      "epoch 4890: test loss is 0.048949647694826126\n",
      "epoch 4900: train loss is 0.0422171546289554\n",
      "epoch 4900: test loss is 0.04096462205052376\n",
      "epoch 4910: train loss is 0.039802380479299106\n",
      "epoch 4910: test loss is 0.04051410034298897\n",
      "epoch 4920: train loss is 0.039982350972982555\n",
      "epoch 4920: test loss is 0.05249189957976341\n",
      "epoch 4930: train loss is 0.038105556884637244\n",
      "epoch 4930: test loss is 0.04005187749862671\n",
      "epoch 4940: train loss is 0.03655359707772732\n",
      "epoch 4940: test loss is 0.0428050234913826\n",
      "epoch 4950: train loss is 0.03728800997711145\n",
      "epoch 4950: test loss is 0.045028094202280045\n",
      "epoch 4960: train loss is 0.039732008885878786\n",
      "epoch 4960: test loss is 0.045249152928590775\n",
      "epoch 4970: train loss is 0.03696973793781721\n",
      "epoch 4970: test loss is 0.04153609648346901\n",
      "epoch 4980: train loss is 0.03825532587674948\n",
      "epoch 4980: test loss is 0.045236632227897644\n",
      "epoch 4990: train loss is 0.0396753059556851\n",
      "epoch 4990: test loss is 0.04513295367360115\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_loss = 100\n",
    "for ep in range(num_epochs):\n",
    "    model.zero_grad()\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    # Initialise hidden state\n",
    "    model.hidden = model.init_hidden()\n",
    "    model.train()\n",
    "   # scheduler.step()\n",
    "    for X, y, pc in train_loader:\n",
    "       # print(y[0])\n",
    "        X, y, pc = X.cuda(), y.cuda(), pc.cuda()\n",
    "    \n",
    "        y_pred = model(X)\n",
    "        loss_1 = loss_fn(y_pred, y)\n",
    " #       loss_2 = loss_fn(y_pred[:,:-1,:], y_pred[:,1:,:])\n",
    "#         loss_3 = 0\n",
    "#         a,b,_ = y_pred.shape\n",
    "# #         for i in range(a):\n",
    "# #             for j in range(b):\n",
    "#         for i in range(b):\n",
    "#             pc_pred = coma(y_pred[:,i,:])\n",
    "\n",
    "#             loss_3 += loss_fn(pc_pred.view(y_pred.shape[0], -1), pc[:,i,:])\n",
    "            \n",
    "# #                 loss_2 += loss_fn(pc_pred.view(-1), pc[i,j])\n",
    "#         loss_3 = loss_3/b\n",
    "        loss = loss_1 \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    train_loss_history.append(train_loss/len(train_loader))\n",
    "    \n",
    "    model.eval()\n",
    "    for X, y, pc in test_loader:\n",
    "       # print(y[0])\n",
    "        X, y, pc = X.cuda(), y.cuda(), pc.cuda()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X)\n",
    "            a,b,_ = y_pred.shape\n",
    "#           loss_2 = loss_fn(y_pred[:,:-1,:], y_pred[:,1:,:])\n",
    "          #  pc_pred = coma(y_pred[i])\n",
    "           # loss_2 += loss_fn(pc_pred.view(4, -1), pc[i])\n",
    "#             loss_3 = 0\n",
    "#             for i in range(b):\n",
    "#                 pc_pred = coma(y_pred[:,i,:])\n",
    "#                 loss_3 += loss_fn(pc_pred.view(y_pred.shape[0], -1), pc[:,i,:])\n",
    "            \n",
    "#             loss_3 = loss_3/b\n",
    "            loss_1 = loss_fn(y_pred, y)\n",
    "           # loss_2 = loss_2/a\n",
    "            loss = loss_1\n",
    "        test_loss += loss.detach().cpu().numpy()\n",
    "    test_loss_history.append(test_loss/len(test_loader))\n",
    "    \n",
    "    if ep % 10 == 0:\n",
    "    #    print('Epoch:', ep,'LR:', scheduler.get_lr())\n",
    "        print(\"epoch {}: train loss is {}\".format(ep, train_loss/len(train_loader)))\n",
    "        print(\"epoch {}: test loss is {}\".format(ep, test_loss/len(test_loader)))\n",
    "        \n",
    "    if test_loss/len(test_loader) < best_loss:\n",
    "        torch.save(model, checkpoint_dir+'result')\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "times = list(range(len(train_loss_history)))\n",
    "\n",
    "fig = plt.figure()\n",
    "#ax = fig.add_subplot(111)\n",
    "#ax.plot(times, train_loss_history)\n",
    "#ax.plot(times, test_loss_history)\n",
    "p1, = plt.plot(times, train_loss_history)\n",
    "p2, = plt.plot(times, test_loss_history)\n",
    "plt.legend([p1, p2], ['train losss', 'valid loss'])\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\" loss\")\n",
    "plt.savefig(checkpoint_dir+'result.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 100, 10)\n"
     ]
    }
   ],
   "source": [
    "###test\n",
    "feat = np.empty([0,100,10])\n",
    "model = torch.load('/home/hyewon/Downloads/deep_dress/pytorch_coma-master/LSTM_results/0820_10/'+'result').cuda()\n",
    "model.eval()\n",
    "results=[]\n",
    "error_10 = []\n",
    "for X, y, pc in test_loader:\n",
    "   # print(y[0])\n",
    "    X= X.cuda()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X).detach().cpu().numpy()\n",
    "        print(y_pred.shape)\n",
    "        feat = np.concatenate((feat, y_pred), axis = 0)\n",
    "        error_10.append(np.abs(y_pred-y.cpu().numpy()))\n",
    "    #    np.append(feat, y_pred, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 100, 8)\n"
     ]
    }
   ],
   "source": [
    "###test\n",
    "feat = np.empty([0,100,8])\n",
    "model = torch.load('/home/hyewon/Downloads/deep_dress/pytorch_coma-master/LSTM_results/0814_8_2/'+'result').cuda()\n",
    "model.eval()\n",
    "results=[]\n",
    "error = []\n",
    "for X, y, pc in test_loader:\n",
    "   # print(y[0])\n",
    "    X= X.cuda()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X).detach().cpu().numpy()\n",
    "        print(y_pred.shape)\n",
    "        feat = np.concatenate((feat, y_pred), axis = 0)\n",
    "        error.append(np.abs(y_pred-y.cpu().numpy()))\n",
    "    #    np.append(feat, y_pred, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f997a506748>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_8 = error[0].flatten()\n",
    "me_8 = np.max(a_8)\n",
    "tep = np.zeros(100)\n",
    "for i in range(100):\n",
    "    tep[i] = a_8[a_8<me_8*i/100].shape[0]/3200\n",
    "plt.plot(tep)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f997a55ae48>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_10 = error_10[0].flatten()\n",
    "me_10 = np.max(a_10)\n",
    "tep_10 = np.zeros(100)\n",
    "for i in range(100):\n",
    "    tep_10[i] = a_10[a_10<me_10*i/100].shape[0]/a_10.shape[0]\n",
    "plt.plot(tep_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "times = np.asarray(list(range(100)))/100\n",
    "p1, = plt.plot(times, tep_10)\n",
    "p2, = plt.plot(times, tep)\n",
    "plt.legend([p1, p2], ['shape = (10,)', 'shape = (8,)'])\n",
    "plt.ylabel(\"Percentage of vertex\")\n",
    "plt.xlabel(\"Percentage of error\")\n",
    "plt.savefig(checkpoint_dir+'result.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 8)\n"
     ]
    }
   ],
   "source": [
    "feature = np.empty([0,8])\n",
    "for i in range(4):\n",
    "    feature = np.concatenate((feature, feat[i,:,:].squeeze()), axis = 0)\n",
    "print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.asarray(results[:-1], dtype = 'float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73360,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.load(\"./8_train.npz\", allow_pickle = True)#loading data\n",
    "clothes_feature = data[\"cloth\"]\n",
    "nd = clothes_feature[0].flatten()\n",
    "for i in range(1,14):\n",
    "    temp = clothes_feature[i].flatten()\n",
    "    nd = np.concatenate((nd, temp), axis = 0)\n",
    "print(nd.shape)\n",
    "plt.hist(nd, bins = 50)\n",
    "plt.xlabel(\"Absolute value\")\n",
    "plt.ylabel(\"Number of vertex\")\n",
    "plt.savefig('hist_1.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72088,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.load(\"./16_train.npz\", allow_pickle = True)#loading data\n",
    "clothes_feature = data[\"cloth\"]\n",
    "nd_16 = clothes_feature[0].flatten()\n",
    "for i in range(1,14):\n",
    "    idx = np.random.randint(0,high=16, size = 8)\n",
    "   # print(clothes_feature[i][:,[1,2]].shape)\n",
    "    temp = clothes_feature[i][:,idx].flatten()\n",
    "    nd_16 = np.concatenate((nd_16, temp), axis = 0)\n",
    "print(nd_16.shape)\n",
    "plt.hist(nd_16, bins = 50)\n",
    "plt.xlabel(\"Absolute value\")\n",
    "plt.ylabel(\"Number of vertex\")\n",
    "plt.savefig('hist_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = range(10)\n",
    "sample_8 = np.random.choice(nd, size = 10000, replace = False)\n",
    "sample_16 = np.random.choice(nd_16, size = 10000, replace = False)\n",
    "a = np.zeros(300)\n",
    "b = np.zeros(300)\n",
    "for i in range(100):\n",
    "    a[i] = a[a<i/10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(sample_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = np.zeros((400,10))\n",
    "feature[0:100] = feat[0]\n",
    "feature[100:200] = feat[1]\n",
    "feature[200:300] = feat[2]\n",
    "feature[300:400] = feat[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.77622070e+01,  4.06008053e+00, -1.42568257e-03,  4.70943069e+00,\n",
       "        1.06809158e+01, -2.05184740e-04,  1.82912103e-04,  5.66401148e+00,\n",
       "       -1.57745075e-04,  5.16194582e+00])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Coma\n",
    "from config_parser import read_config\n",
    "import argparse\n",
    "from psbody.mesh import Mesh, MeshViewers\n",
    "import mesh_operations\n",
    "from transform import Normalize\n",
    "from data import ComaDataset\n",
    "parser = argparse.ArgumentParser(description='Pytorch Trainer for Convolutional Mesh Autoencoders')\n",
    "config = read_config('./default.cfg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scipy_to_torch_sparse(scp_matrix):\n",
    "    values = scp_matrix.data\n",
    "    indices = np.vstack((scp_matrix.row, scp_matrix.col))\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = scp_matrix.shape\n",
    "\n",
    "    sparse_tensor = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "    return sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_flag = config['eval']\n",
    "lr = config['learning_rate']\n",
    "lr_decay = config['learning_rate_decay']\n",
    "weight_decay = config['weight_decay']\n",
    "total_epochs = config['epoch']\n",
    "workers_thread = config['workers_thread']\n",
    "opt = config['optimizer']\n",
    "batch_size = config['batch_size']\n",
    "val_losses, accs, durations = [], [], []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyewon/Downloads/deep_dress/pytorch_coma-master/mesh_operations.py:231: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  coeffs_v[3 * i:3 * i + 3] = np.linalg.lstsq(A, nearest_v)[0]\n",
      "/home/hyewon/Downloads/deep_dress/pytorch_coma-master/mesh_operations.py:235: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  tmp_coeffs = np.linalg.lstsq(A, target.v[i])[0]\n"
     ]
    }
   ],
   "source": [
    "template_mesh = Mesh(filename=\"./template/template.obj\")\n",
    "M, A, D, U = mesh_operations.generate_transform_matrices(template_mesh, config['downsampling_factors'])\n",
    "\n",
    "D_t = [scipy_to_torch_sparse(d).to(device) for d in D]\n",
    "U_t = [scipy_to_torch_sparse(u).to(device) for u in U]\n",
    "A_t = [scipy_to_torch_sparse(a).to(device) for a in A]\n",
    "num_nodes = [len(M[i].v) for i in range(len(M))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = config['data_dir']\n",
    "\n",
    "normalize_transform = Normalize()\n",
    "dataset = ComaDataset(data_dir, dtype='train', split='sliced', split_term='sliced', pre_transform=normalize_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "coma = Coma(dataset, config, D_t, U_t, A_t, num_nodes).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('./results/0819/checkpoint_300.pt')\n",
    "start_epoch = checkpoint['epoch_num']\n",
    "coma.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './LSTM_results/0820_10/reconstruction_results'\n",
    "p = np.load('/home/hyewon/Downloads/deep_dress/Dress dataset/0507/0507_poses.npz')\n",
    "pi = 3.1415926\n",
    "M_1 = transforms3d.euler.euler2mat(-pi/2,0,0,axes='sxyz')\n",
    "trans = np.zeros((3,1))\n",
    "M_1 = np.concatenate((M_1, trans), axis = 1)\n",
    "hel = np.array([[0,0,0,1]])\n",
    "M_1 = np.concatenate((M_1, hel), axis = 0)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n",
      "(4179, 3)\n"
     ]
    }
   ],
   "source": [
    "test_tensor = torch.Tensor(gt[0])\n",
    "i = 0\n",
    "for t in test_tensor:\n",
    "    i += 1\n",
    "    t = t.unsqueeze(0).cuda()\n",
    "    save_out = coma(t).cpu().detach().numpy()\n",
    "    print(save_out.shape)\n",
    "    save_obj(save_out , template_mesh.f+1, os.path.join(output_dir, 'test_'+str(i)+'.obj'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n",
      "torch.Size([10])\n",
      "(4179, 4)\n"
     ]
    }
   ],
   "source": [
    "for i,f in enumerate(feat):\n",
    "    f = torch.Tensor(feat[i]).cuda()\n",
    "    save_out = coma(f.unsqueeze(0)).cpu().detach().numpy()\n",
    "    save_out = save_out*dataset.std.numpy()+dataset.mean.numpy()\n",
    "    \n",
    "    rot = p['poses'][i+2]\n",
    "    trans = p['trans'][i+2]\n",
    "    axis_ang = rot[0:3]\n",
    "    a = np.linalg.norm(axis_ang)\n",
    "    b = np.divide(axis_ang,a)\n",
    "    euler = transforms3d.euler.axangle2euler(b,a,axes='sxyz')\n",
    "    rot_matrix = transforms3d.euler.euler2mat(euler[0], euler[1], euler[2], axes='sxyz')\n",
    "    trans = np.expand_dims(np.asarray(trans), axis = 1)\n",
    "    M = np.concatenate((rot_matrix, trans), axis = 1)\n",
    "    M = np.concatenate((M, hel), axis = 0)\n",
    "    \n",
    "    temp = np.ones((save_out.shape[0], 1))\n",
    "    pc_body = np.concatenate((save_out,temp), axis = 1)\n",
    "    M = M_1.dot(M)\n",
    " #   M_inv = np.linalg.inv(M)\n",
    "    pc_w = (M.dot(pc_body.T)).T\n",
    "    print(pc_w.shape)\n",
    "    \n",
    "    save_obj(pc_w , template_mesh.f+1, os.path.join(output_dir, 'test_'+str(i)+'.obj'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09543929, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.02027742, 0.        ,\n",
       "        0.03657463, 0.04969572, 0.00565663, 0.09194449, 0.25195611,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
